@misc{trsh,
  author = {\underline{Santiago Castro} and Mat{\'{i}}as Mansilla},
  title = {Resoluci{\'{o}}n del problema de recolecci{\'{o}}n de residuos utilizando algoritmos gen{\'{e}}ticos},
  year = {2014},
  month = oct,
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/bryant1410/trsh}}
}

@thesis{castro2015deteccion,
  title={Humor Detection in Spanish Tweets (`Detección de humor en textos en español')},
  author={\underline{Santiago Castro} and Cubero, Matías},
  institution={Universidad de la República},
  type={Bachelor's thesis},
  month=may,
  year={2015}
}

@inproceedings{castro2016joke,
  title={Is This a Joke? Detecting Humor in Spanish Tweets},
  author={\underline{Santiago Castro} and Cubero, Matías and Garat, Diego and Moncecchi, Guillermo},
  booktitle={IBERAMIA},
  pages={139--150},
  month=nov,
  year={2016},
  note={\color{red}{Best Paper Award \nth{2} place}}
}

@misc{castro2016esto,
  title={Is This a Joke? Detecting Humor in Tweets (``¿Esto es un chiste? Detección de humor en tuits'')},
  author={\underline{Santiago Castro}},
  editor={9º Foro de Lenguas de ANEP},
  pages={115--126},
  month=dec,
  year={2016}
}

@misc{xmartlabs-2017-bender,
  author = {Mathias Claassen and \underline{Santiago Castro}},
  title = {Bender: Easily craft fast Neural Networks on {iOS}!},
  year = {2017},
  month = jun,
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://xmartlabs.github.io/Bender/}}
}

@misc{castro-2017-fast-krippendorff,
  author = {\underline{Santiago Castro}},
  title = {Fast {K}rippendorff: Fast computation of {K}rippendorff's alpha agreement measure},
  month = sep,
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/pln-fing-udelar/fast-krippendorff}}
}

@incollection{rosa2017retuyt,
  title={RETUYT in TASS 2017: Sentiment Analysis for Spanish Tweets using SVM and CNN},
  author={Rosá, Aiala and Chiruzzo, Luis and Etcheverry, Mathias and \underline{Santiago Castro}},
  booktitle={TASS @ SEPLN},
  month=sep,
  year={2017}
}

@incollection{castro2018crowd,
  title={A crowd-annotated spanish corpus for humor analysis},
  author={\underline{Santiago Castro} and Chiruzzo, Luis and Rosá, Aiala and Garat, Diego and Moncecchi, Guillermo},
  booktitle={SocialNLP @ ACL},
  pages={7--11},
  month=jul,
  year={2018}
}

@incollection{castro2018high,
  title={A High Coverage Method for Automatic False Friends Detection for Spanish and Portuguese},
  author={\underline{Santiago Castro} and Bonanata, Jairo and Rosá, Aiala},
  booktitle={VarDial @ COLING},
  pages={29--36},
  month=aug,
  year={2018}
}

@incollection{castro2018overview,
  title={Overview of the HAHA Task: Humor Analysis based on Human Annotation at IberEval 2018},
  author={\underline{Santiago Castro} and Chiruzzo, Luis and Rosá, Aiala},
  booktitle={IberEval @ SEPLN},
  volume={2150},
  pages={187--194},
  month=sep,
  year={2018}
}

@inproceedings{mustard,
  title = "Towards Multimodal Sarcasm Detection (An  \_Obviously\_ Perfect Paper)",
  author = "\underline{Santiago Castro}  and
    Hazarika, Devamanyu  and
    P{\'e}rez-Rosas, Ver{\'o}nica  and
    Zimmermann, Roger  and
    Mihalcea, Rada  and
    Poria, Soujanya",
  booktitle = "ACL",
  month = jul,
  year = "2019"
}

@incollection{haha2019,
  author      = "Chiruzzo, Luis and \underline{Santiago Castro} and Etcheverry, Mathias and Garat, Diego and Prada, Juan Jos{\'e} and Ros{\'a}, Aiala",
  title       = "{Overview of HAHA at IberLEF 2019: Humor Analysis based on Human Annotation}",
  booktitle   = "IberLEF @ SEPLN",
  month       = sep,
  year        = "2019",
}

@inproceedings{castro-etal-2020-lifeqa,
  title = "{L}ife{QA}: A Real-life Dataset for Video Question Answering",
  author = "\underline{Santiago Castro}  and
    Azab, Mahmoud  and
    Stroud, Jonathan  and
    Noujaim, Cristina  and
    Wang, Ruoyao  and
    Deng, Jia  and
    Mihalcea, Rada",
  booktitle = "LREC",
  month = may,
  year = "2020",
%  address = "Marseille, France",
%  publisher = "European Language Resources Association",
  url = "https://aclanthology.org/2020.lrec-1.536",
  pages = "4352--4358",
%  language = "English",
%  ISBN = "979-10-95546-34-4",
}

@inproceedings{chiruzzo-etal-2020-haha,
  title = "{HAHA} 2019 Dataset: A Corpus for Humor Analysis in {S}panish",
  author = "Chiruzzo, Luis  and
    \underline{Santiago Castro}  and
    Ros{\'a}, Aiala",
  booktitle = "LREC",
  month = may,
  year = "2020",
%  address = "Marseille, France",
%  publisher = "European Language Resources Association",
  url = "https://aclanthology.org/2020.lrec-1.628",
  pages = "5106--5112",
%  language = "English",
%  ISBN = "979-10-95546-34-4",
}

@article{PLN6394,
  author = {Luis Chiruzzo and \underline{Santiago Castro} and Santiago Góngora and Aiala Rosa and J. A. Meaney and Rada Mihalcea},
  title = {Overview of HAHA at IberLEF 2021: Detecting, Rating and Analyzing Humor in Spanish},
  journal = {Procesamiento del Lenguaje Natural},
  volume = {67},
  number = {0},
  month = sep,
  year = {2021},
  keywords = {},
  abstract = {We present the results of HAHA at IberLEF 2021: Humor Analysis ba-sed on Human Annotation. This year’s edition of the competition includes the two classic tasks of humor detection and rating, plus two novel tasks of humor logic mechanism and target classiﬁcation. We describe the corpus created for the challenge, the competition phases, the submitted systems and the main results obtained.},
  issn = {1989-7553},
  pages = {257--268}
}

@inproceedings{reasons,
  title = "WhyAct: Identifying Action Reasons in Lifestyle Vlogs",
  author = "Oana Ignat and \underline{Santiago Castro} and Hanwen Miao and Weiji Li and Rada Mihalcea",
  booktitle = "EMNLP",
  month = nov,
  year = "2021",
  publisher = "Association for Computational Linguistics",
}

@article{10.1145/3495211,
  author = {Ignat, Oana and \underline{Santiago Castro} and Zhou, Yuhang and Bao, Jiajun and Shan, Dandan and Mihalcea, Rada},
  title = {When Did It Happen? Duration-Informed Temporal Localization of Narrated Actions in Vlogs},
  month = feb,
  year = {2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1551-6857},
  url = {https://doi.org/10.1145/3495211},
  doi = {10.1145/3495211},
  abstract = {We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previous methods, and leads to improvements over previous work for the task of temporal action localization.},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  keywords = {action duration, vlogs, multimodal processing, video processing, action temporal localization, natural language processing}
}

@inproceedings{fitclip,
  author = {\underline{Santiago Castro} and Heilbron, Fabian Caba},
  title = {{FitCLIP}: Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks},
  booktitle = {BMVC},
  month = nov,
  year = {2022}
}

@inproceedings{castro-etal-2022-fill,
  title = "{FIBER}: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework",
  author = "\underline{Santiago Castro} and
    Wang, Ruoyao  and
    Huang, Pingxuan  and
    Stewart, Ian  and
    Ignat, Oana  and
    Liu, Nan  and
    Stroud, Jonathan C.  and
    Mihalcea, Rada",
  booktitle = "ACL",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.acl-long.209",
  pages = "2925--2940",
  abstract = "We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER {--} a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework. The fill-in-the-blanks setting tests a model{'}s understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the video and the surrounding text. The FIBER benchmark does not share the weaknesses of the current state-of-the-art language-informed video understanding tasks, namely: (1) video question answering using multiple-choice questions, where models perform relatively well because they exploit linguistic biases in the task formulation, thus making our framework challenging for the current state-of-the-art systems to solve; and (2) video captioning, which relies on an open-ended evaluation framework that is often inaccurate because system answers may be perceived as incorrect if they differ in form from the ground truth. The FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/.",
}

@inproceedings{castro-etal-2022-in-the-wild,
    title = "In-the-Wild Video Question Answering",
    author = "\underline{Santiago Castro}  and
      Deng, Naihao  and
      Huang, Pingxuan  and
      Burzo, Mihai G.  and
      Mihalcea, Rada",
    booktitle = "COLING",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.496",
    pages = "5613--5635",
}

@article{PLN6447,
	author = {Aiala Rosá and Luis Chiruzzo and Lucía Bouza and Alina Dragonetti and \underline{Santiago Castro} and Mathias Etcheverry and Santiago Góngora and Santiago Goycoechea and Juan Machado and Guillermo Moncecchi and Juan José Prada and Dina Wonsever},
	title = {Overview of QuALES at IberLEF 2022: Question Answering Learning from Examples in Spanish},
	journal = {Procesamiento del Lenguaje Natural},
	volume = {69},
	number = {0},
    month = sep,
	year = {2022},
	keywords = {},
	abstract = {We present the results of the QuALES task, which addresses the problem of Extractive Question Answering from texts. For both training and evaluation we use the QuALES corpus, a corpus of Uruguayan media news about the Covid-19 pandemic and related topics. We describe the systems developed by seven participants, all of them based on different BERT-like language models. The best results were obtained using the multilingual RoBERTa model pre-trained with SQUAD-Es-V2, with a fine tuning on the QuALES corpus.},
	issn = {1989-7553},
	url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6447},
	pages = {273--280}
}

@inproceedings{phenaki,
title={Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions},
author={Ruben Villegas and Mohammad Babaeizadeh and Pieter-Jan Kindermans and Hernan Moraldo and Han Zhang and Mohammad Taghi Saffar and \underline{Santiago Castro} and Julius Kunze and Dumitru Erhan},
booktitle={ICLR},
month=may,
year={2023},
url={https://openreview.net/forum?id=vOEXS39nOF}
}

@incollection{probing-clip,
    title = "Scalable Performance Analysis for Vision-Language Models",
    author = "\underline{Santiago Castro}  and
      Ignat, Oana  and
      Mihalcea, Rada",
    booktitle = "*SEM @ ACL",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.starsem-1.26",
    pages = "284--294",
    abstract = "Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at this https URL and can be used with other multimodal models and benchmarks.",
}
